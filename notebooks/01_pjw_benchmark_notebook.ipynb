{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d7fb756",
   "metadata": {},
   "source": [
    "### HOW TO MAP FLOODWATER FROM RADAR IMAGERY USING SEMANTIC SEGMENTATION - BENCHMARK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf02b0e",
   "metadata": {},
   "source": [
    "https://www.drivendata.co/blog/detect-floodwater-benchmark/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf50648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import rasterio\n",
    "from pandas_path import path\n",
    "\n",
    "# This is where our downloaded images and metadata live locally\n",
    "DATA_PATH = Path(\"/home/paul/Documents/drivendata/stac-overflow/data/external/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da2ba6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata = pd.read_csv(\n",
    "    DATA_PATH / \"flood-training-metadata.csv\", parse_dates=[\"scene_start\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c8ca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ceb577",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1fb7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata[\"feature_path\"] = (\n",
    "    str(DATA_PATH / \"train_features\")\n",
    "    / train_metadata.image_id.path.with_suffix(\".tif\").path\n",
    ")\n",
    "\n",
    "train_metadata[\"label_path\"] = (\n",
    "    str(DATA_PATH / \"train_labels\")\n",
    "    / train_metadata.chip_id.path.with_suffix(\".tif\").path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8ff08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = train_metadata.feature_path[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef48ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(image_path) as img:\n",
    "    metadata = img.meta\n",
    "    bounds = img.bounds\n",
    "    data = img.read(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aff25aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5576c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_img(matrix):\n",
    "    \"\"\"\n",
    "    Returns a scaled (H, W, D) image that is visually inspectable.\n",
    "    Image is linearly scaled between min_ and max_value, by channel.\n",
    "\n",
    "    Args:\n",
    "        matrix (np.array): (H, W, D) image to be scaled\n",
    "\n",
    "    Returns:\n",
    "        np.array: Image (H, W, 3) ready for visualization\n",
    "    \"\"\"\n",
    "    # Set min/max values\n",
    "    min_values = np.array([-23, -28, 0.2])\n",
    "    max_values = np.array([0, -5, 1])\n",
    "\n",
    "    # Reshape matrix\n",
    "    w, h, d = matrix.shape\n",
    "    matrix = np.reshape(matrix, [w * h, d]).astype(np.float64)\n",
    "\n",
    "    # Scale by min/max\n",
    "    matrix = (matrix - min_values[None, :]) / (\n",
    "        max_values[None, :] - min_values[None, :]\n",
    "    )\n",
    "    matrix = np.reshape(matrix, [w, h, d])\n",
    "\n",
    "    # Limit values to 0/1 interval\n",
    "    return matrix.clip(0, 1)\n",
    "\n",
    "\n",
    "def create_false_color_composite(path_vv, path_vh):\n",
    "    \"\"\"\n",
    "    Returns a S1 false color composite for visualization.\n",
    "\n",
    "    Args:\n",
    "        path_vv (str): path to the VV band\n",
    "        path_vh (str): path to the VH band\n",
    "\n",
    "    Returns:\n",
    "        np.array: image (H, W, 3) ready for visualization\n",
    "    \"\"\"\n",
    "    # Read VV/VH bands\n",
    "    with rasterio.open(path_vv) as vv:\n",
    "        vv_img = vv.read(1)\n",
    "    with rasterio.open(path_vh) as vh:\n",
    "        vh_img = vh.read(1)\n",
    "\n",
    "    # Stack arrays along the last dimension\n",
    "    s1_img = np.stack((vv_img, vh_img), axis=-1)\n",
    "\n",
    "    # Create false color composite\n",
    "    img = np.zeros((512, 512, 3), dtype=np.float32)\n",
    "    img[:, :, :2] = s1_img.copy()\n",
    "    img[:, :, 2] = s1_img[:, :, 0] / s1_img[:, :, 1]\n",
    "\n",
    "    return scale_img(img)\n",
    "\n",
    "\n",
    "def display_random_chip(random_state):\n",
    "    \"\"\"\n",
    "    Plots a 3-channel representation of VV/VH polarizations as a single chip (image 1).\n",
    "    Overlays a chip's corresponding water label (image 2).\n",
    "\n",
    "    Args:\n",
    "        random_state (int): random seed used to select a chip\n",
    "\n",
    "    Returns:\n",
    "        plot.show(): chip and labels plotted with pyplot\n",
    "    \"\"\"\n",
    "    f, ax = plt.subplots(1, 2, figsize=(9, 9))\n",
    "\n",
    "    # Select a random chip from train_metadata\n",
    "    random_chip = train_metadata.chip_id.sample(random_state=random_state).values[0]\n",
    "    chip_df = train_metadata[train_metadata.chip_id == random_chip]\n",
    "\n",
    "    # Extract paths to image files\n",
    "    vv_path = chip_df[chip_df.polarization == \"vv\"].feature_path.values[0]\n",
    "    vh_path = chip_df[chip_df.polarization == \"vh\"].feature_path.values[0]\n",
    "    label_path = chip_df.label_path.values[0]\n",
    "\n",
    "    # Create false color composite\n",
    "    s1_img = create_false_color_composite(vv_path, vh_path)\n",
    "\n",
    "    # Visualize features\n",
    "    ax[0].imshow(s1_img)\n",
    "    ax[0].set_title(\"S1 Chip\", fontsize=14)\n",
    "\n",
    "    # Load water mask\n",
    "    with rasterio.open(label_path) as lp:\n",
    "        lp_img = lp.read(1)\n",
    "\n",
    "    # Mask missing data and 0s for visualization\n",
    "    label = np.ma.masked_where((lp_img == 0) | (lp_img == 255), lp_img)\n",
    "\n",
    "    # Visualize water label\n",
    "    ax[1].imshow(s1_img)\n",
    "    ax[1].imshow(label, cmap=\"cool\", alpha=1)\n",
    "    ax[1].set_title(\"S1 Chip with Water Label\", fontsize=14)\n",
    "\n",
    "    plt.tight_layout(pad=5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e833412",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_random_chip(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16769079",
   "metadata": {},
   "source": [
    "\"You'll notice that some images contain high floodwater coverage, while others have little to no coverage. Water spread may be concentrated in one location or spread out, depending on a variety of surface characteristics like topography, terrain, and ground cover. Missing pixels are displayed in yellow.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330c9ea8",
   "metadata": {},
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b23096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(9)  # set a seed for reproducibility\n",
    "\n",
    "# Sample 3 random floods for validation set\n",
    "flood_ids = train_metadata.flood_id.unique().tolist()\n",
    "val_flood_ids = random.sample(flood_ids, 3)\n",
    "val_flood_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eea972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = train_metadata[train_metadata.flood_id.isin(val_flood_ids)]\n",
    "train = train_metadata[~train_metadata.flood_id.isin(val_flood_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cc82af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for pivoting out paths by chip\n",
    "def get_paths_by_chip(image_level_df):\n",
    "    \"\"\"\n",
    "    Returns a chip-level dataframe with pivoted columns\n",
    "    for vv_path and vh_path.\n",
    "\n",
    "    Args:\n",
    "        image_level_df (pd.DataFrame): image-level dataframe\n",
    "\n",
    "    Returns:\n",
    "        chip_level_df (pd.DataFrame): chip-level dataframe\n",
    "    \"\"\"\n",
    "    paths = []\n",
    "    for chip, group in image_level_df.groupby(\"chip_id\"):\n",
    "        vv_path = group[group.polarization == \"vv\"][\"feature_path\"].values[0]\n",
    "        vh_path = group[group.polarization == \"vh\"][\"feature_path\"].values[0]\n",
    "        paths.append([chip, vv_path, vh_path])\n",
    "    return pd.DataFrame(paths, columns=[\"chip_id\", \"vv_path\", \"vh_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c5b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features from labels\n",
    "val_x = get_paths_by_chip(val)\n",
    "val_y = val[[\"chip_id\", \"label_path\"]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "train_x = get_paths_by_chip(train)\n",
    "train_y = train[[\"chip_id\", \"label_path\"]].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dd7855",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2985ab7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9a7f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "class FloodDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Reads in images, transforms pixel values, and serves a\n",
    "    dictionary containing chip ids, image tensors, and\n",
    "    label masks (where available).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x_paths, y_paths=None, transforms=None):\n",
    "        self.data = x_paths\n",
    "        self.label = y_paths\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Loads a 2-channel image from a chip-level dataframe\n",
    "        img = self.data.loc[idx]\n",
    "        with rasterio.open(img.vv_path) as vv:\n",
    "            vv_path = vv.read(1)\n",
    "            # vv_path = (vv_path - np.nanmin(vv_path))/(np.nanmax(vv_path) - np.nanmin(vv_path))\n",
    "        with rasterio.open(img.vh_path) as vh:\n",
    "            vh_path = vh.read(1)\n",
    "            # vh_path = (vh_path - np.nanmin(vh_path))/(np.nanmax(vh_path) - np.nanmin(vh_path))\n",
    "\n",
    "        x_arr = np.stack([vv_path, vh_path], axis=-1)\n",
    "\n",
    "        # Min-max normalization\n",
    "        min_norm = -77\n",
    "        max_norm = 26\n",
    "        x_arr = np.clip(x_arr, min_norm, max_norm)\n",
    "        x_arr = (x_arr - min_norm) / (max_norm - min_norm)\n",
    "\n",
    "        # Load label if available - training only\n",
    "        if self.label is not None:\n",
    "            label_path = self.label.loc[idx].label_path\n",
    "            with rasterio.open(label_path) as lp:\n",
    "                y_arr = lp.read(1)\n",
    "\n",
    "            # Apply same data augmentations to sample and label\n",
    "            if self.transforms:\n",
    "                transformed = self.transforms(image=x_arr, mask=y_arr)\n",
    "                x_arr = transformed[\"image\"]\n",
    "                y_arr = transformed[\"mask\"]\n",
    "\n",
    "            x_arr = np.transpose(x_arr, [2, 0, 1])\n",
    "\n",
    "            sample = {\"chip_id\": img.chip_id, \"chip\": x_arr, \"label\": y_arr}\n",
    "        else:  # No labels - validation set only\n",
    "            if self.transforms:\n",
    "                x_arr = self.transforms(image=x_arr)[\"image\"]\n",
    "\n",
    "            x_arr = np.transpose(x_arr, [2, 0, 1])\n",
    "            sample = {\"chip_id\": img.chip_id, \"chip\": x_arr}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28982cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations\n",
    "# These transformations will be passed to our model class\n",
    "training_transformations = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.RandomCrop(256, 256),\n",
    "        albumentations.RandomRotate90(),\n",
    "        albumentations.HorizontalFlip(),\n",
    "        albumentations.VerticalFlip(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cd9040",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XEDiceLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Computes (0.5 * CrossEntropyLoss) + (0.5 * DiceLoss).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.xe = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    def forward(self, pred, true):\n",
    "        valid_pixel_mask = true.ne(255)  # valid pixel mask\n",
    "\n",
    "        # Cross-entropy loss\n",
    "        temp_true = torch.where((true == 255), 0, true)  # cast 255 to 0 temporarily\n",
    "        xe_loss = self.xe(pred, temp_true)\n",
    "        xe_loss = xe_loss.masked_select(valid_pixel_mask).mean()\n",
    "\n",
    "        # Dice loss\n",
    "        pred = torch.softmax(pred, dim=1)[:, 1]\n",
    "        pred = pred.masked_select(valid_pixel_mask)\n",
    "        true = true.masked_select(valid_pixel_mask)\n",
    "        dice_loss = 1 - (2.0 * torch.sum(pred * true)) / (torch.sum(pred + true) + 1e-7)\n",
    "\n",
    "        return (0.5 * xe_loss) + (0.5 * dice_loss)\n",
    "\n",
    "\n",
    "def intersection_and_union(pred, true):\n",
    "    \"\"\"\n",
    "    Calculates intersection and union for a batch of images.\n",
    "\n",
    "    Args:\n",
    "        pred (torch.Tensor): a tensor of predictions\n",
    "        true (torc.Tensor): a tensor of labels\n",
    "\n",
    "    Returns:\n",
    "        intersection (int): total intersection of pixels\n",
    "        union (int): total union of pixels\n",
    "    \"\"\"\n",
    "    valid_pixel_mask = true.ne(255)  # valid pixel mask\n",
    "    true = true.masked_select(valid_pixel_mask).to(\"cpu\")\n",
    "    pred = pred.masked_select(valid_pixel_mask).to(\"cpu\")\n",
    "\n",
    "    # Intersection and union totals\n",
    "    intersection = np.logical_and(true, pred)\n",
    "    union = np.logical_or(true, pred)\n",
    "    return intersection.sum(), union.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7821551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import segmentation_models_pytorch as smp\n",
    "import tqdm\n",
    "\n",
    "\n",
    "class FloodModel(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super(FloodModel, self).__init__()\n",
    "        self.hparams.update(hparams)\n",
    "        self.save_hyperparameters()\n",
    "        self.backbone = self.hparams.get(\"backbone\", \"resnet34\")\n",
    "        self.weights = self.hparams.get(\"weights\", \"imagenet\")\n",
    "        self.learning_rate = self.hparams.get(\"lr\", 1e-3)\n",
    "        self.max_epochs = self.hparams.get(\"max_epochs\", 1000)\n",
    "        self.min_epochs = self.hparams.get(\"min_epochs\", 6)\n",
    "        self.patience = self.hparams.get(\"patience\", 4)\n",
    "        self.num_workers = self.hparams.get(\"num_workers\", 2)\n",
    "        self.batch_size = self.hparams.get(\"batch_size\", 32)\n",
    "        self.x_train = self.hparams.get(\"x_train\")\n",
    "        self.y_train = self.hparams.get(\"y_train\")\n",
    "        self.x_val = self.hparams.get(\"x_val\")\n",
    "        self.y_val = self.hparams.get(\"y_val\")\n",
    "        self.output_path = self.hparams.get(\"output_path\", \"model-outputs\")\n",
    "        self.gpu = self.hparams.get(\"gpu\", False)\n",
    "        self.transform = training_transformations\n",
    "\n",
    "        # Where final model will be saved\n",
    "        self.output_path = Path.cwd() / self.output_path\n",
    "        self.output_path.mkdir(exist_ok=True)\n",
    "\n",
    "        # Track validation IOU globally (reset each epoch)\n",
    "        self.intersection = 0\n",
    "        self.union = 0\n",
    "\n",
    "        # Instantiate datasets, model, and trainer params\n",
    "        self.train_dataset = FloodDataset(\n",
    "            self.x_train, self.y_train, transforms=self.transform\n",
    "        )\n",
    "        self.val_dataset = FloodDataset(self.x_val, self.y_val, transforms=None)\n",
    "        self.model = self._prepare_model()\n",
    "        self.trainer_params = self._get_trainer_params()\n",
    "\n",
    "    ## Required LightningModule methods ##\n",
    "\n",
    "    def forward(self, image):\n",
    "        # Forward pass\n",
    "        return self.model(image)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Switch on training mode\n",
    "        self.model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "\n",
    "        # Load images and labels\n",
    "        x = batch[\"chip\"]\n",
    "        y = batch[\"label\"].long()\n",
    "        if self.gpu:\n",
    "            x, y = x.cuda(non_blocking=True), y.cuda(non_blocking=True)\n",
    "\n",
    "        # Forward pass\n",
    "        preds = self.forward(x)\n",
    "\n",
    "        # Calculate training loss\n",
    "        criterion = XEDiceLoss()\n",
    "        xe_dice_loss = criterion(preds, y)\n",
    "\n",
    "        # Log batch xe_dice_loss\n",
    "        self.log(\n",
    "            \"xe_dice_loss\",\n",
    "            xe_dice_loss,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        return xe_dice_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Switch on validation mode\n",
    "        self.model.eval()\n",
    "        torch.set_grad_enabled(False)\n",
    "\n",
    "        # Load images and labels\n",
    "        x = batch[\"chip\"]\n",
    "        y = batch[\"label\"].long()\n",
    "        if self.gpu:\n",
    "            x, y = x.cuda(non_blocking=True), y.cuda(non_blocking=True)\n",
    "\n",
    "        # Forward pass & softmax\n",
    "        preds = self.forward(x)\n",
    "        preds = torch.softmax(preds, dim=1)[:, 1]\n",
    "        preds = (preds > 0.5) * 1\n",
    "\n",
    "        # Calculate validation IOU (global)\n",
    "        intersection, union = intersection_and_union(preds, y)\n",
    "        self.intersection += intersection\n",
    "        self.union += union\n",
    "\n",
    "        # Log batch IOU\n",
    "        batch_iou = intersection / union\n",
    "        self.log(\n",
    "            \"iou\", batch_iou, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        return batch_iou\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # DataLoader class for training\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # DataLoader class for validation\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=0,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Define optimizer\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        # Define scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode=\"max\", factor=0.5, patience=self.patience\n",
    "        )\n",
    "        scheduler = {\n",
    "            \"scheduler\": scheduler,\n",
    "            \"interval\": \"epoch\",\n",
    "            \"monitor\": \"val_loss\",\n",
    "        }  # logged value to monitor\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # Calculate IOU at end of epoch\n",
    "        epoch_iou = self.intersection / self.union\n",
    "\n",
    "        # Reset metrics before next epoch\n",
    "        self.intersection = 0\n",
    "        self.union = 0\n",
    "\n",
    "        # Log epoch validation IOU\n",
    "        self.log(\"val_loss\", epoch_iou, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return epoch_iou\n",
    "\n",
    "    ## Convenience Methods ##\n",
    "\n",
    "    def _prepare_model(self):\n",
    "        unet_model = smp.Unet(\n",
    "            encoder_name=self.backbone,\n",
    "            encoder_weights=self.weights,\n",
    "            in_channels=2,\n",
    "            classes=2,\n",
    "        )\n",
    "        if self.gpu:\n",
    "            unet_model.cuda()\n",
    "        return unet_model\n",
    "\n",
    "    def _get_trainer_params(self):\n",
    "        # Define callback behavior\n",
    "        checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "            dirpath=self.output_path,\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"max\",\n",
    "            verbose=True,\n",
    "        )\n",
    "        early_stop_callback = pl.callbacks.early_stopping.EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=(self.patience * 3),\n",
    "            mode=\"max\",\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        # Specify where TensorBoard logs will be saved\n",
    "        self.log_path = Path.cwd() / self.hparams.get(\"log_path\", \"tensorboard-logs\")\n",
    "        self.log_path.mkdir(exist_ok=True)\n",
    "        logger = pl.loggers.TensorBoardLogger(self.log_path, name=\"benchmark-model\")\n",
    "\n",
    "        trainer_params = {\n",
    "            \"callbacks\": [checkpoint_callback, early_stop_callback],\n",
    "            \"max_epochs\": self.max_epochs,\n",
    "            \"min_epochs\": self.min_epochs,\n",
    "            \"default_root_dir\": self.output_path,\n",
    "            \"logger\": logger,\n",
    "            \"gpus\": None if not self.gpu else 1,\n",
    "            \"fast_dev_run\": self.hparams.get(\"fast_dev_run\", False),\n",
    "            \"num_sanity_val_steps\": self.hparams.get(\"val_sanity_checks\", 0),\n",
    "        }\n",
    "        return trainer_params\n",
    "\n",
    "    def fit(self):\n",
    "        # Set up and fit Trainer object\n",
    "        self.trainer = pl.Trainer(**self.trainer_params)\n",
    "        self.trainer.fit(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5874dbe5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    # Required hparams\n",
    "    \"x_train\": train_x,\n",
    "    \"x_val\": val_x,\n",
    "    \"y_train\": train_y,\n",
    "    \"y_val\": val_y,\n",
    "    # Optional hparams\n",
    "    \"backbone\": \"resnet34\",\n",
    "    \"weights\": \"imagenet\",\n",
    "    \"lr\": 5e-5,\n",
    "    \"min_epochs\": 6,\n",
    "    \"max_epochs\": 1000,\n",
    "    \"patience\": 4,\n",
    "    \"batch_size\": 16,\n",
    "    \"num_workers\": 16,\n",
    "    \"val_sanity_checks\": 0,\n",
    "    \"fast_dev_run\": False,\n",
    "    \"output_path\": \"model-outputs\",\n",
    "    \"log_path\": \"tensorboard_logs\",\n",
    "    \"gpu\": torch.cuda.is_available(),\n",
    "}\n",
    "flood_model = FloodModel(hparams=hparams)\n",
    "flood_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a156d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_model.trainer_params[\"callbacks\"][0].best_model_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
